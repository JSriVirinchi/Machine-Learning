{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment 7",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTU-_zOIpJVs"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.layers.experimental import preprocessing\n",
        "from tensorflow.keras.layers import LSTM, Dense, GRU\n",
        "from tensorflow.keras.layers import BatchNormalization, Dropout, Embedding\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "import string\n",
        "import os\n",
        "import datetime\n",
        "import time"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tquNBURGw0Rx"
      },
      "source": [
        "# Load the extension for tensorboard\n",
        "%load_ext tensorboard"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FA5O1fyUxA-K"
      },
      "source": [
        "Get and read dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tToD4Tslw5S3",
        "outputId": "b16c0d6f-f395-40d3-ce47-38daa1085ba0"
      },
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
        "#Read with correct format\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1122304/1115394 [==============================] - 0s 0us/step\n",
            "1130496/1115394 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YuJswOmsxTbW",
        "outputId": "72d8b66f-fa7b-47d2-e1ce-f56fa89b2888"
      },
      "source": [
        "#Find total number of characters\n",
        "print(len(text))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1115394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DxEbwfvYxjx6",
        "outputId": "522f4616-03e7-449d-dc3e-ae241df9457c"
      },
      "source": [
        "#Check how the data looks like, first 1000 characters\n",
        "print(text[:1000])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor citizens, the patricians good.\n",
            "What authority surfeits on would relieve us: if they\n",
            "would yield us but the superfluity, while it were\n",
            "wholesome, we might guess they relieved us humanely;\n",
            "but they think we are too dear: the leanness that\n",
            "afflicts us, the object of our misery, is as an\n",
            "inventory to particularise their abundance; our\n",
            "sufferance is a gain to them Let us revenge this with\n",
            "our pikes, ere we become rakes: for the gods know I\n",
            "speak this in hunger for bread, not in thirst for revenge.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78dXQqdvxuiw",
        "outputId": "ddbb7263-dbff-4249-90a6-2ff9cf18a0e3"
      },
      "source": [
        "# Finding unique characters\n",
        "vocab = sorted(set(text))\n",
        "print(len(vocab))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEQYASgIyHPR"
      },
      "source": [
        "Vectorization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUR-mNks1raq"
      },
      "source": [
        "Stings to indices and indices to strings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6xZiJ82x_UU"
      },
      "source": [
        "#Converting strings to integers\n",
        "\n",
        "# Mapping strings from vocabulary to indices\n",
        "ids_from_chars = preprocessing.StringLookup(vocabulary=list(vocab), mask_token=None)\n",
        "# To change char ids to strings\n",
        "chars_from_ids = preprocessing.StringLookup(vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Ld4zmwvz4Y7"
      },
      "source": [
        "#Converts ids back to strings\n",
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YK3_tBUxzCnJ",
        "outputId": "5025daee-14fb-4eca-daa3-42df46c2edea"
      },
      "source": [
        "#all_ids is a numpy array consisting of ids of all the characters\n",
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "print(all_ids)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([19 48 57 ... 46  9  1], shape=(1115394,), dtype=int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcZCXajW1wHl"
      },
      "source": [
        "Making dataset into batches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCtom0UmzHEF"
      },
      "source": [
        "#from_tensor_slices converts text vectors into stream of character indices\n",
        "#ids_dataset contains all the character ids\n",
        "#this is done to later shift the characters\n",
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWpSpv760IAr",
        "outputId": "ce0147fd-2307-4e99-86ed-88cd22e42886"
      },
      "source": [
        "# text is divided into sequences of length 100 \n",
        "# each sequence and its target have the same size but the target is shifted by 1 char towards right\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "examples_per_epoch"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11043"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s33OoHi61aBL",
        "outputId": "930188e3-b64d-4389-c544-09c07654f0c2"
      },
      "source": [
        "# creating batches and storing into sequences\n",
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "print(\"All letters in first sequence\\n\")\n",
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq))\n",
        "\n",
        "print(\"\\nConverting chars into string and printing first sequence\\n\")\n",
        "for seq in sequences.take(1):\n",
        "  print(text_from_ids(seq).numpy())"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All letters in first sequence\n",
            "\n",
            "tf.Tensor(\n",
            "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
            " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
            " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
            " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
            " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
            " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
            " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
            " b'o' b'u' b' '], shape=(101,), dtype=string)\n",
            "\n",
            "Converting chars into string and printing first sequence\n",
            "\n",
            "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RUsrAxtQ2L2F",
        "outputId": "6d0a9ea7-b540-49d6-c258-30b060aa1032"
      },
      "source": [
        "#input and target with a difference of 1 char (target shifted right). So it aligns the input and the label for each timestep\n",
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "print(split_input_target(list(\"Sample text\")))\n",
        "\n",
        "# map input and label for each sequence\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(['S', 'a', 'm', 'p', 'l', 'e', ' ', 't', 'e', 'x'], ['a', 'm', 'p', 'l', 'e', ' ', 't', 'e', 'x', 't'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8CikaztM3hsu",
        "outputId": "f06f9f66-d9ef-469c-d048-889b4cbafb12"
      },
      "source": [
        "# First sequence input and label\n",
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "    print(\"Target:\", text_from_ids(target_example).numpy())"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTePycJf4gWM"
      },
      "source": [
        "# Gated recurrent units "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNoaE3ww4Sds",
        "outputId": "a6ad2a6e-44d5-4b4c-9026-4cc0229c9cab"
      },
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset_1 = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset_1"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PrefetchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVWyuREH5RNy"
      },
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab) +1\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdslQo4C5X_z"
      },
      "source": [
        "#Creating layers using Keras subclass\n",
        "class Model_1(tf.keras.Model):\n",
        "  #All the sublayers are created inside init() method\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    \n",
        "    #Embedding is the input layer. A lookup table that will map each character id to vector with embedding-dimensions \n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.dense_1 = tf.keras.layers.Dense(1024, activation = 'relu')\n",
        "\n",
        "    #GRU layer\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units, return_sequences = True, return_state = True)\n",
        "\n",
        "    #Batch Normalisation\n",
        "    self.batch = tf.keras.layers.BatchNormalization()\n",
        "\n",
        "    #The output layer with vocab_size outputs. \n",
        "    self.dense_2 = tf.keras.layers.Dense(vocab_size, activation = 'relu')\n",
        "\n",
        "  #call method will automatically run build the first time it is called\n",
        "  def call(self, inputs, states = None, return_state = False, training = False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training = training)\n",
        "    x = self.dense_1(x, training = training)\n",
        "\n",
        "    #initial_state is the list of initial state tensors to be passed to the first call of the cell\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "\n",
        "    x, states = self.gru(x, initial_state = states, training = training)\n",
        "    x = self.batch(x)\n",
        "    x = self.dense_2(x, training = training)\n",
        "\n",
        "    #whether to return the last state in addition to the output\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-gJ7ZsV53dk"
      },
      "source": [
        "model_GRU = Model_1(\n",
        "    vocab_size = len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim = embedding_dim,\n",
        "    rnn_units = rnn_units\n",
        ")"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "slOa4xmz6ahi",
        "outputId": "aa00c813-c2cf-486c-cad8-2fe944dc5bbf"
      },
      "source": [
        "# Shape of the input\n",
        "for input_example_batch, target_example_batch in dataset_1.take(1):\n",
        "    example_batch_predictions = model_GRU(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \" -> (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 66)  -> (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7IDCvluW6bSO",
        "outputId": "13572a99-2e5a-4f2b-870e-629515160b64"
      },
      "source": [
        "model_GRU.summary()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        multiple                  16896     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                multiple                  263168    \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    multiple                  6297600   \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo multiple                  4096      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              multiple                  67650     \n",
            "=================================================================\n",
            "Total params: 6,649,410\n",
            "Trainable params: 6,647,362\n",
            "Non-trainable params: 2,048\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gm66-k5p8Fre"
      },
      "source": [
        "Compile"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gzOeyQ_6tDK"
      },
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "model_GRU.compile(optimizer = 'adam', loss = loss, metrics= ['accuracy'])"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0SHoxixA7_QZ",
        "outputId": "171c74cf-0ce2-471b-9d99-679e644c9fcf"
      },
      "source": [
        "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
        "mean_loss = example_batch_loss.numpy().mean()\n",
        "\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" -> (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:\", mean_loss)\n",
        "print(\"Mean loss exp \",tf.exp(mean_loss).numpy())"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (64, 100, 66)  -> (batch_size, sequence_length, vocab_size)\n",
            "Mean loss: 4.18916\n",
            "Mean loss exp  65.96735\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQiG7sKk8Emk"
      },
      "source": [
        "#Saving the checkpoints\n",
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "log_dir = \"logs/fit/GRU/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix, save_weights_only=True)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gisck_jj_U6c",
        "outputId": "abe04e70-7ee6-4054-d3d4-1ac6efd860a8"
      },
      "source": [
        "history_GRU = model_GRU.fit(dataset_1, epochs=50, callbacks=[checkpoint_callback, tensorboard_callback])"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "172/172 [==============================] - 36s 187ms/step - loss: 2.2846 - accuracy: 0.3781\n",
            "Epoch 2/50\n",
            "172/172 [==============================] - 34s 186ms/step - loss: 1.8570 - accuracy: 0.4762\n",
            "Epoch 3/50\n",
            "172/172 [==============================] - 33s 185ms/step - loss: 1.6498 - accuracy: 0.5297\n",
            "Epoch 4/50\n",
            "172/172 [==============================] - 33s 185ms/step - loss: 1.5226 - accuracy: 0.5611\n",
            "Epoch 5/50\n",
            "172/172 [==============================] - 33s 185ms/step - loss: 1.4326 - accuracy: 0.5828\n",
            "Epoch 6/50\n",
            "172/172 [==============================] - 33s 185ms/step - loss: 1.3662 - accuracy: 0.5992\n",
            "Epoch 7/50\n",
            "172/172 [==============================] - 33s 185ms/step - loss: 1.3061 - accuracy: 0.6149\n",
            "Epoch 8/50\n",
            "172/172 [==============================] - 33s 185ms/step - loss: 1.2460 - accuracy: 0.6314\n",
            "Epoch 9/50\n",
            "172/172 [==============================] - 33s 184ms/step - loss: 1.1863 - accuracy: 0.6490\n",
            "Epoch 10/50\n",
            "172/172 [==============================] - 33s 184ms/step - loss: 1.1215 - accuracy: 0.6685\n",
            "Epoch 11/50\n",
            "172/172 [==============================] - 33s 185ms/step - loss: 1.0534 - accuracy: 0.6894\n",
            "Epoch 12/50\n",
            "172/172 [==============================] - 33s 185ms/step - loss: 0.9824 - accuracy: 0.7124\n",
            "Epoch 13/50\n",
            "172/172 [==============================] - 33s 185ms/step - loss: 0.9103 - accuracy: 0.7359\n",
            "Epoch 14/50\n",
            "172/172 [==============================] - 33s 185ms/step - loss: 0.8424 - accuracy: 0.7576\n",
            "Epoch 15/50\n",
            "172/172 [==============================] - 33s 185ms/step - loss: 0.7809 - accuracy: 0.7775\n",
            "Epoch 16/50\n",
            "172/172 [==============================] - 33s 185ms/step - loss: 0.7236 - accuracy: 0.7951\n",
            "Epoch 17/50\n",
            "172/172 [==============================] - 33s 185ms/step - loss: 0.6753 - accuracy: 0.8104\n",
            "Epoch 18/50\n",
            "172/172 [==============================] - 34s 185ms/step - loss: 0.6321 - accuracy: 0.8234\n",
            "Epoch 19/50\n",
            "172/172 [==============================] - 33s 185ms/step - loss: 0.5948 - accuracy: 0.8345\n",
            "Epoch 20/50\n",
            "172/172 [==============================] - 34s 185ms/step - loss: 0.5606 - accuracy: 0.8420\n",
            "Epoch 21/50\n",
            "172/172 [==============================] - 33s 184ms/step - loss: 0.5266 - accuracy: 0.8488\n",
            "Epoch 22/50\n",
            "172/172 [==============================] - 33s 184ms/step - loss: 0.4888 - accuracy: 0.8596\n",
            "Epoch 23/50\n",
            "172/172 [==============================] - 33s 184ms/step - loss: 0.4571 - accuracy: 0.8697\n",
            "Epoch 24/50\n",
            "172/172 [==============================] - 33s 185ms/step - loss: 0.4328 - accuracy: 0.8772\n",
            "Epoch 25/50\n",
            "172/172 [==============================] - 33s 185ms/step - loss: 0.4159 - accuracy: 0.8821\n",
            "Epoch 26/50\n",
            "172/172 [==============================] - 33s 184ms/step - loss: 0.4032 - accuracy: 0.8856\n",
            "Epoch 27/50\n",
            "172/172 [==============================] - 33s 185ms/step - loss: 0.3924 - accuracy: 0.8884\n",
            "Epoch 28/50\n",
            "172/172 [==============================] - 33s 185ms/step - loss: 0.3818 - accuracy: 0.8917\n",
            "Epoch 29/50\n",
            "172/172 [==============================] - 33s 184ms/step - loss: 0.3709 - accuracy: 0.8943\n",
            "Epoch 30/50\n",
            "172/172 [==============================] - 33s 185ms/step - loss: 0.3607 - accuracy: 0.8978\n",
            "Epoch 31/50\n",
            "172/172 [==============================] - 33s 185ms/step - loss: 0.3491 - accuracy: 0.9012\n",
            "Epoch 32/50\n",
            "172/172 [==============================] - 33s 184ms/step - loss: 0.3414 - accuracy: 0.9036\n",
            "Epoch 33/50\n",
            "172/172 [==============================] - 33s 184ms/step - loss: 0.3342 - accuracy: 0.9052\n",
            "Epoch 34/50\n",
            "172/172 [==============================] - 33s 185ms/step - loss: 0.3293 - accuracy: 0.9068\n",
            "Epoch 35/50\n",
            "172/172 [==============================] - 33s 185ms/step - loss: 0.3281 - accuracy: 0.9068\n",
            "Epoch 36/50\n",
            "172/172 [==============================] - 33s 184ms/step - loss: 0.3252 - accuracy: 0.9074\n",
            "Epoch 37/50\n",
            "172/172 [==============================] - 33s 184ms/step - loss: 0.3268 - accuracy: 0.9067\n",
            "Epoch 38/50\n",
            "172/172 [==============================] - 33s 184ms/step - loss: 0.3198 - accuracy: 0.9088\n",
            "Epoch 39/50\n",
            "172/172 [==============================] - 33s 184ms/step - loss: 0.3123 - accuracy: 0.9112\n",
            "Epoch 40/50\n",
            "172/172 [==============================] - 33s 184ms/step - loss: 0.3034 - accuracy: 0.9143\n",
            "Epoch 41/50\n",
            "172/172 [==============================] - 33s 184ms/step - loss: 0.2996 - accuracy: 0.9152\n",
            "Epoch 42/50\n",
            "172/172 [==============================] - 33s 184ms/step - loss: 0.2982 - accuracy: 0.9156\n",
            "Epoch 43/50\n",
            "172/172 [==============================] - 33s 185ms/step - loss: 0.3018 - accuracy: 0.9140\n",
            "Epoch 44/50\n",
            "172/172 [==============================] - 33s 184ms/step - loss: 0.3011 - accuracy: 0.9141\n",
            "Epoch 45/50\n",
            "172/172 [==============================] - 33s 185ms/step - loss: 0.2980 - accuracy: 0.9149\n",
            "Epoch 46/50\n",
            "172/172 [==============================] - 33s 185ms/step - loss: 0.2986 - accuracy: 0.9147\n",
            "Epoch 47/50\n",
            "172/172 [==============================] - 33s 185ms/step - loss: 0.2954 - accuracy: 0.9156\n",
            "Epoch 48/50\n",
            "172/172 [==============================] - 33s 185ms/step - loss: 0.2954 - accuracy: 0.9155\n",
            "Epoch 49/50\n",
            "172/172 [==============================] - 33s 185ms/step - loss: 0.2893 - accuracy: 0.9176\n",
            "Epoch 50/50\n",
            "172/172 [==============================] - 33s 184ms/step - loss: 0.2854 - accuracy: 0.9189\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptKb-h5L_tbg"
      },
      "source": [
        "# Text generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FytfJvw_g8-"
      },
      "source": [
        "#To generate the text, a loop is ran and each time the model is called some text is passed wih the internal state. The model returns a predicton for the next character and its \n",
        "#new state which is again passed to continue generating text.\n",
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.5):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    #Sparse Tensor enables an efficient storage and processing of tensors that contain a lot of zero values\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        #Values is a 1D tensor containing all nonzero values\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        #Indices is a 2D tensor containing the indices of nonzero values\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "geBOHaNzAUhZ"
      },
      "source": [
        "one_step_model = OneStep(model_GRU, chars_from_ids, ids_from_chars)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GqujCvGPAYqP",
        "outputId": "32a6e658-fdbb-43d3-a19c-90946d9f5eb6"
      },
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['very happy for you!'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(500):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "very happy for you! May I neel you go\n",
            "what?\n",
            "\n",
            "BIONDELLO:\n",
            "I canNay.\n",
            "\n",
            "HORTENSIO:\n",
            "Well, 'tis a litland cha: which Warwick xanchos' loves.\n",
            "\n",
            "MOPSA:\n",
            "I was promised them aga, poH: but, by your leave,\n",
            "I shall not notess, call not us gone.\n",
            "\n",
            "LUCENTIO:\n",
            "Provost! most whatxare shall, I will prove so, being plead\n",
            "XetortaXENT:\n",
            "You sjudx'd at my master, who never keeps\n",
            "The Apto through by when he made fa: Angelo\n",
            "Doth mine men put to death a dine Kate leave\n",
            "To hear Conspira calls your penitence, I\n",
            "find his troHPaIr of his Time king \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 3.7162413597106934\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8C_n4WnpAeEN",
        "outputId": "6504405b-3df0-4072-8d4e-410058073511"
      },
      "source": [
        "tf.saved_model.save(one_step_model, 'one_step')\n",
        "one_step_reloaded = tf.saved_model.load('one_step')"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x7f8cd1e05310>, because it is not built.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x7f8cd1e05310>, because it is not built.\n",
            "WARNING:absl:Found untraced functions such as gru_cell_layer_call_and_return_conditional_losses, gru_cell_layer_call_fn, gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses, gru_cell_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: one_step/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: one_step/assets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FM-y69qTAiea"
      },
      "source": [
        "input_word = 'My name is Virinchi'"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2SG55sItApyT",
        "outputId": "6453005e-01df-47ca-cd46-80152d0dfda4"
      },
      "source": [
        "states = None\n",
        "next_char = tf.constant([input_word])\n",
        "result = [next_char]\n",
        "r = len(input_word) + 10\n",
        "for n in range(r):\n",
        "  next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "My name is Virinching beasts in ,\n",
            "Which issured \n"
          ]
        }
      ]
    }
  ]
}